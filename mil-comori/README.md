# Morphotype analysis - weakly supervised learning for virtual cell quantification

## Description of the training script for general application

Uses a multi-instance learning approach to deduce virtual cells (or cellular archetypes) that can be relevant for specific classifications. The script responsible for most of the training is `scripts/train_stack.py`, which uses the models, metrics and data generators specified in `scripts/networks.py`, `scripts/metrics.py` and `scripts/data_generators.py`, respectively. 

We use the hierarchical format HDF5 for each one of the datasets. Particularly, we structure these files such that a file is composed of multiple groups, each of which corresponds to an array with $n$ rows (the number of detected cells) and $f$ columns (the features characterising each cell). Given that we are interested in multiple types of cells (RBC and WBC), we have two separate hdf5 files specified in such a way. The names of the groups in the HDF5 file (slide IDs) are important as this is what is used to match groups of cells between different files.

The Morphotype analysis script (through `train_stack.py`) allows multiple cell datasets to be provided (`dataset_path`), as well as additional tabular datasets (`other_dataset_path`), provided they are CSV files where the first row corresponds to the slide ID. If a variable is categorical (i.e. most elements are non-numerical) this will be converted to a one-hot encoded variable. Morphotype analysis also allows for multiple objectives through the specificaiton of multiple `labels_path`. More parameters can be accessed through `python3 train_stack.py --help`. An example is provided below:

```
    python3 scripts/train_stack.py \
        --n_virtual_cells 25 \
        --n_classes 2 \
        --dataset_path datasets/wbc.h5 \
        --dataset_path datasets/rbc.h5 \
        --labels_path labels/A.csv \
        --labels_path labels/B.csv \
        --number_of_steps 10000 \
        --batch_size 32 \
        --number_of_cells 500 \
        --learning_rate 0.01 \
        --n_folds 5 \
        --weight_decay 0.2 \
        --other_dataset_path datasets/other_data.csv \
        --model_path models/model \
        --excluded_ids A B C DE F \
        --median_impute \
        --min_cells 50 
```

## Obtaining Morphotypes for different conditions

If training using `Snakefile` (recommended to generate all the necessary output files for the rest of the analysis), it is necessary to have a version of `R` installed (v3.6.1 was used in this work) with `tidyverse==1.3.1`, `caret==6.0-83` and `pROC==1.18.0`. 

**If you want to proceed with the analysis in `analysis-plotting` without running this step, you can download the output from Morphotype analysis as made available in https://figshare.com/articles/software/MILe-ViCe_output/19369391. If you want to run Morphotype analysis yourself, please see below.**

### Training and validating condition predictions with Morphotype analysis

All necessary Python packages are in `requirements.txt`. This was developed and tested using Python 3.6.8 and on 8GB RAM + 12GB VRAM on CentOS Linux 8 (kernel: Linux 4.18.0-240.22.1.el8_3.x86_64). To run Morphotype analysis one should follow these steps:

1. Install the packages in `requirements.txt` (i.e. `pip install -r requirements`). Alternatively, if you use `conda`, you can also use `environment.yaml` to setup a new environment (`conda env create -f environment.yml`) and activate it (`conda activate pytorch`)
2. If you haven't done so: download the data necessary for training and validation, availabe in https://figshare.com/articles/dataset/MILe-ViCe_training_and_validation_datasets/19372292. This is the equivalent of aggregating the output from [Haemorasis](https://github.com/josegcpa/haemorasis) using `scripts/aggregate_datasets.py` but, considering the large volume of data generated by Haemorasis, we provide a Figshare dataset link instead.
3. Run the `snakemake` pipeline which coordinates training runs with different numbers morphotypes assumptions. To do so, we provide two options: `sh cross_validate_all.sh configs/config-subset.yaml` or, in case you have access to a LSF high performance computing environment, `sh cross_validate_all_cluster.sh configs/config-subset.yaml`. This step, without the access to a cluster, can take over a day, while cluster access cuts this down to approximately 3 hours
4. Run the `snakemake` pipeline which coordinates validation using `snakemake -s Snakefile-validation-subset -c 1 --latency-wait 120`, where the `-c` flags specifies how many cores should be used. 
